# Hardware Implementation

NVIDIA GPU架构围绕可扩展的多线程流式多处理器（SM）阵列构建。 当主机CPU上的CUDA程序调用内核网格时，将枚举网格块并将其分配给具有可用执行容量的多处理器。 线程块的线程在一个多处理器上并发执行，并且多个线程块可以在一个多处理器上并发执行。 当线程块终止时，在空出的多处理器上启动新块。

多处理器旨在同时执行数百个线程。 为了管理如此大量的线程，它采用了SIMT架构中描述的称为SIMT（单指令，多线程）的独特架构。 这些指令是流水线的，以便在单个线程中利用指令级并行性，以及通过硬件多线程中详述的同步硬件多线程来广泛地利用线程级并行性。 与CPU核心不同，它们按顺序发布，并且没有分支预测，也没有推测执行。

 SIMT架构和硬件多线程描述了流式多处理器的架构特性，这些特性适用于所有设备。 计算能力3.x，计算能力5.x，计算能力6.x和计算能力7.x分别为计算能力3.x，5.x，6.x和7.x的设备提供细节。

NVIDIA GPU架构使用little-endian表示。

### SIMT Architecture

多处理器以32个并行线程（称为warps）的组为单位创建，管理，调度和执行线程。 组成warp的各个线程在同一个程序地址处一起启动，但是它们有自己的指令地址计数器和寄存器状态，因此可以自由地分支和执行。 术语warp源于编织，这是第一个并行线程技术。 半翘曲是经线的第一个或第二个半部分。 四分之一扭曲是经线的第一，第二，第三或第四季度。

当为多处理器提供一个或多个要执行的线程块时，它会将它们分成warp，每个warp由warp调度程序调度执行。 块被分割成warp的方式总是一样的; 每个warp包含连续的，增加的线程ID的线程，第一个warp包含线程0.线程层次结构描述线程ID如何与块中的线程索引相关。

warp一次执行一条公共指令，因此当warp的所有32个线程在其执行路径上达成一致时，实现了完全的效率。 如果warp的线程通过数据相关的条件分支发散，则warp执行所采用的每个分支路径，禁用不在该路径上的线程。 分支分歧仅在经线内发生; 不同的warp独立执行，无论它们是执行公共代码路径还是不相交的代码路径。

SIMT体系结构类似于SIMD\(单指令、多数据\)向量组织，因为单指令控制多个处理元素。一个关键的区别是SIMD向量组织向软件公开了SIMD宽度，而SIMT指令指定了单个线程的执行和分支行为。与SIMD向量机不同，SIMT使程序员能够为独立的标量线程编写线程级并行代码，并为协调线程编写数据并行代码。为了正确性，程序员可以基本上忽略SIMT行为；然而，通过注意代码很少需要扭曲中的线程分叉，可以实现实质性的性能改进。实际上，这类似于传统代码中高速缓存行的角色:在设计正确性时可以安全地忽略高速缓存行的大小，但在设计最高性能时，必须在代码结构中加以考虑。另一方面，向量架构需要软件将负载合并成向量，并手动管理差异。

 在Volta之前，warps使用在warp中所有32个线程之间共享的单个程序计数器以及指定warp的活动线程的活动掩码。 结果，来自不同区域或不同执行状态的相同warp的线程不能相互发信号或交换数据，并且需要细粒度共享由锁或互斥锁保护的数据的算法很容易导致死锁，这取决于哪个warp 竞争线程来自。

从Volta架构开始，独立线程调度允许线程之间的完全并发，无论是否为warp。 通过独立线程调度，GPU维护每个线程的执行状态，包括程序计数器和调用堆栈，并且可以以每线程粒度执行，以更好地利用执行资源或允许一个线程等待数据到 由另一个人制作。 计划优化器确定如何将来自同一warp的活动线程组合到SIMT单元中。 这保留了SIMT执行的高吞吐量，就像之前的NVIDIA GPU一样，但具有更大的灵活性：线程现在可以在子扭曲粒度上分叉和重新收敛。

如果开发人员对先前硬件体系结构的warp-synchronicity1做出假设，那么独立线程调度可能导致参与执行代码的线程集相当不同。 特别是，应重新考虑任何经线同步代码（例如无同步，内部经线缩减）以确保与Volta及更高版本的兼容性。 有关详细信息，请参阅Compute Capability 7.x.

注意

参与当前指令的warp的线程称为活动线程，而不在当前指令上的线程处于非活动状态（禁用）。 线程可以由于各种原因而处于非活动状态，包括早于其warp的其他线程退出，采用与warp当前执行的分支路径不同的分支路径，或者是线程数不是的块的最后一个线程 经线尺寸的倍数。

如果由warp执行的非原子指令写入warp的多个线程的全局或共享内存中的相同位置，则该位置发生的序列化写入的数量会根据设备的计算能力而变化 （参见Compute Capability 3.x，Compute Capability 5.x，Compute Capability 6.x和Compute Capability 7.x），以及哪个线程执行最终写操作是未定义的。

如果warp执行的原子指令读取，修改并写入warp的多个线程的全局内存中的相同位置，则每个读取/修改/写入该位置并且它们都被序列化，但是 它们发生的顺序是不确定的。

### Hardware Multithreading

由多处理器处理的每个warp的执行上下文（程序计数器，寄存器等）在warp的整个生命周期内保持在片上。 因此，从一个执行上下文切换到另一个执行上下文没有成本，并且在每个指令发布时，warp调度程序选择具有准备好执行其下一条指令的线程的warp（warp的活动线程）并向这些线程发出指令 。

特别地，每个多处理器具有一组在warp之间分区的32位寄存器，以及在线程块之间分区的并行数据高速缓存或共享存储器。

对于给定内核，可以在多处理器上驻留和一起处理的块和warp的数量取决于内核使用的寄存器和共享内存的数量以及多处理器上可用的寄存器和共享内存的数量。 每个多处理器还有最大驻留块数和最大驻留warp数。 这些限制以及多处理器上可用的寄存器和共享内存量是设备计算能力的函数，在附录计算功能中给出。 如果每个多处理器没有足够的寄存器或共享内存来处理至少一个块，则内核将无法启动。

块中的warp总数如下：

ceil \( T W s i z e , 1 \)

* _T_ is the number of threads per block,
* _Wsize_ is the warp size, which is equal to 32,
* ceil\(x, y\) is equal to x rounded up to the nearest multiple of y.

在块中分配的寄存器总数和共享内存总量记录在CUDA工具包中提供的CUDA占用计算器中。

