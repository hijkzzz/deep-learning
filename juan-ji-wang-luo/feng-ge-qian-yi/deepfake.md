# Fast Face-swap

## 介绍

> [Fast Face-swap Using Convolutional Neural Networks](https://arxiv.org/abs/1611.09577)

我们考虑图像中的人脸交换问题，其中输入身份被转换成目标身份，同时保持姿态、面部表情和光照。为了执行这种映射，我们使用卷积神经网络来从他/她的照片的非结构化集合中捕获目标身份的外观。这种方法是通过根据风格转换来构建面部交换问题来实现的，其中的目标是以另一个图像的风格来保留一个图像。基于这一领域的最新进展，我们设计了一种新的损失函数，使网络能够产生高度逼真的结果。通过将神经网络与简单的预处理和后处理步骤相结合，我们的目标是在没有用户输入的情况下使面部交换实时工作。

![](../../.gitbook/assets/image%20%2855%29.png)

## 方法

![](../../.gitbook/assets/image%20%28130%29.png)

拥有人A的形象，我们希望将他/她的身份转化为人B的身份，同时保持头部和表达以及照明条件的完整。在风格转移方面，我们将输入图像A的姿势和表达视为内容，并输入 图像B的标识是风格。 

我们使用由权重参数化的卷积神经网络来转换内容图像 $$\hat{\mathbf{x}}=f_{\mathbf{W}}(\mathbf{x})$$ 。与以前的工作不同，我们假设我们得到的不是一幅而是一组风格的图像，我们用 $$Y={y_1, ..., y_N}$$ 。这些图像描述了我们想要匹配的身份，并且仅在网络训练期间使用。

我们的系统有两个额外的组件，表面对齐和背景/头发/皮肤分割。 我们假设所有图像（内容和样式）都与正面视图参考面对齐。这是使用仿形变换实现的，该变换将给定图像中的68个面部关键点与参考关键点对齐。使用dlib提取面部关键点。分割用于重新存储输入图像的背景和头发。我们使用了OpenCV 中提供的无缝克隆技术来缝合背景和结果面交换的图像，虽然存在快速且相对准确的分割方法，包括一些基于神经网络的方法。

### Transformation network

![](../../.gitbook/assets/image%20%2835%29.png)

图3中的网络是为128×128输入而设计的，有1M个参数。对于较大的输入，例如256×256或512×512，可以直接推断额外分支的架构。网络输出仅从具有最高分辨率的分支获得。

### Loss functions

对于每个输入图像 $$x$$ ，我们的目标是产生 $$\widehat{x}$$ ，共同小化以下内容和风格损失。 这些损失在19层VGG网络的正规化版本的特征空间中定义。

#### Content loss

内容损失通过VGG语义层距离计算得到，即姿势和表情的损失

![](../../.gitbook/assets/image%20%2813%29.png)

![](../../.gitbook/assets/image%20%28217%29.png)

#### Style loss

即人脸损失

我们的损失函数的灵感来自基于Patch的损失。 $$\Psi\left(\Phi_{l}(\hat{\mathbf{x}})\right)$$ 表示在 $$\Phi_{l}(\hat{\mathbf{x}})$$ 上的每个位置$$H_{l} \times W_{l}$$ 上循环生成的所有Path列表，每个位置提取 $$k \times k$$ 的邻居点。所以提取的Path矩阵为 $$M=\left(H_{l}-k+1\right) \times\left(W_{l}-k+1\right)$$ ，每个path的大小为 $$C_{l} \times k \times k$$ 。

对于每个这样的Patch来说，我们发现从Y中提取的Patch中最好的匹配Patch，并最小化它们之间的距离。 作为误差度量，我们使用了正弦距离：

![](../../.gitbook/assets/image%20%28195%29.png)

不过在这里我们只搜索相同位置的Path，但是跨多个样式图像：

![](../../.gitbook/assets/image%20%28105%29.png)

#### Light loss

不幸的是，当仅使用在VGG特征空间中定义的上述损失时，内容图像的照明条件不会保留在生成的图像中。我们通过在我们的目标中引入一个额外的项来解决这个问题，这个项惩罚了照明的变化。为了定义光照惩罚，我们提出了使用预处理网络的特征空间的想法，就像我们使用VGG作为风格和内容一样。如果特征空间表示照明条件的差异，则这种方法将起作用。 VGG不适合这项任务，因为它被训练用于分类对象，其中照明信息并不特别相关。

为了获得理想的光感特性，我们构造了一个小型的siamese卷积神经网络。它被训练为区分具有相同或不同照明条件的图像对。成对的图像始终具有相同的姿势。 我们使用了扩展的耶鲁人脸数据库B \[8\]，其中包含了9个姿势和64个照明条件下的主题的灰度肖像。 照明网络的架构如图4所示。

![](../../.gitbook/assets/image%20%28156%29.png)

我们将把照明网络的最后一层的特征表示为 $$Γ(x)$$ ，并引入下面的损失函数，该函数试图防止生成的图像具有与内容图像x不同的照明条件。 两者都是单通道亮度图像。

![](../../.gitbook/assets/image%20%28187%29.png)

#### Total variation regularization

在约翰逊和其他人的工作之后，我们使用正则化来鼓励空间平滑：

![](../../.gitbook/assets/image%20%28238%29.png)

整体损失为

![](../../.gitbook/assets/image%20%2823%29.png)



