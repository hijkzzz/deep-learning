# Transformer

##  介绍

> [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

主要序列转导模型基于复杂的复发或卷积神经网络，其包括编码器和解码器。 性能最佳的模型还通过注意机制连接编码器和解码器。 我们提出了一种新的简单网络架构，Transformer，完全基于注意机制，完全免除循环和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上更优越，同时更具并行性，并且需要大量的时间来训练。我们的模型在WMT 2014英语 - 德语翻译任务中达到了28.4 BLEU，超过现有的最佳成绩，包括2个以上的BLEU。在WMT 2014英语到法语的翻译任务中，我们的模型建立了一个新的单模型最新BLEU得分为41.8，在8个GPU上续航3.5天，这是最好的模型训练成本的一小部分文献。我们通过将其成功应用于英语选区解析大型和有限的训练数据，表明Transformer能够很好地概括其他任务。

## 方法

大多数有竞争力的神经序列转导模型具有编码器 - 解码器结构。编码器将符号表示的输入序列 $$\left(x_{1}, \dots, x_{n}\right)$$ 映射到连续表示的序列 $$\mathbf{z}=\left(z_{1}, \dots, z_{n}\right)$$ 。 给定 $$z$$ ，解码器然后一次一个元素地生成符号的输出序列 $$\left(y_{1}, \dots, y_{m}\right)$$ 。

Transformer遵循这种整体架构，使用堆叠的自注意和逐点，完全连接的层，用于编码器和解码器，分别如图1的左半部分和右半部分所示。

![](../../.gitbook/assets/image%20%28176%29.png)



### Encoder and Decoder Stacks

#### Encoder

编码器由N = 6个相同层的堆栈组成。 每层都有两层。第一种是多头自我关注机制，第二种是简单的，位置完全连接的前馈网络。 我们在两个子层中的每一个周围使用残差连接\[11\]，然后是层归一化\[1\]（即针对每层的输出跨通道归一化）。也就是说，每个子层的输出是： $$Layernorm(x+\text { Sublayer }(x))$$ 。为了促进这些残差连接，模型中的所有子层以及嵌入层都生成 $$d_{\text { model }}=512$$ 的输出。

#### Decoder

解码器还由N = 6个相同层的堆栈组成。 除了每个编码器层中的两层之外，解码器还插入第三个层，其在编码器堆栈的输出上执行多头注意力机制。 与编码器类似，我们在每个子层周围使用残差连接，然后进层归一化。我们还修改了解码器堆栈中的自注意子层，以防止位置关注后续位置。 此掩码与输出嵌入被一个位置偏移的事实相结合，确保了 $$i$$ 的预测仅依赖于小于 $$i$$ 的位置处的已知输出。

### Attention

注意功能可以被描述为将query和一组key-value对映射到output，其中query，key，value和output都是向量。输出被计算为值的加权总和，其中分配给每个值的权重由query与相应key的兼容性函数来计算。

![](../../.gitbook/assets/image%20%28235%29.png)

#### Scaled Dot-Product Attention

注意力的计算公式如下

![](../../.gitbook/assets/image%20%2868%29.png)

其中Q、K、V分别是query、key、value向量组成的矩阵（这些向量通过embedding与权重矩阵相乘得到）， $$d_{k}$$ 是向量的维度。缩放因子 $$\sqrt{d_{k}}$$ 用于减小点积大小增长对梯度的影响。

另外要说一下新加的attention多加了一个mask，因为训练时的output都是ground truth，这样可以确保预测第i个位置时不会接触到未来的信息

#### Multi-Head Attention

我们使用8个并行的注意力头提升对不同区域的关注

![](../../.gitbook/assets/image%20%28113%29.png)

### Position-wise Feed-Forward Networks

除了注意子层之外，我们编码器和解码器中的每一层都包含一个完全关联的前馈网络，该网络分别且完全相同地应用于每个位置。这包括两个线性变换，其间有ReLU激活。

![](../../.gitbook/assets/image%20%2872%29.png)

### Embeddings and Softmax

与其他序列转导模型类似，我们使用学习嵌入将输入token和输出token转换为 $$d_{\text { model }}$$ 维度的向量。我们还使用通常学习的线性变换和softmax函数将解码器输出转换为预测的下一token的概率。

### Positional Encoding

由于我们的模型不包含重复和没有卷积，为了使模型能够利用序列的顺序，我们必须注入一些关于相对或绝对位置的信息。为此，我们在编码器和解码器堆栈的底部为输入嵌入添加“位置编码”。

![](../../.gitbook/assets/image%20%28192%29.png)

### Why Self-Attention

一个是每层的总计算复杂度。另一个是可以并行化的计算量，用所需的最小顺序操作数来衡量。

第三是网络中远程依赖之间的路径长度。学习长程相关性是许多序列转导任务中的一个关键挑战。影响学习这种依赖性的能力的一个关键因素是前向和后向信号在网络中的总长度。 输入和输出序列中任何位置组合之间的这些路径越短，学习远程依赖性就越容易\[12\]。 因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。

![](../../.gitbook/assets/image%20%28100%29.png)

## 实验

![](../../.gitbook/assets/image%20%2898%29.png)



