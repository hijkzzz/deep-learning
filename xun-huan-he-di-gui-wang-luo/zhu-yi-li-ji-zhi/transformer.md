# Transformer

## 介绍

> [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

主要序列转导模型基于复杂的复发或卷积神经网络，其包括编码器和解码器。 性能最佳的模型还通过注意机制连接编码器和解码器。 我们提出了一种新的简单网络架构，Transformer，完全基于注意机制，完全免除重复和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上更优越，同时更具并行性，并且需要大量的时间来训练。我们的模型在WMT 2014英语 - 德语翻译任务中达到了28.4 BLEU，超过现有的最佳成绩，包括2个以上的BLEU。在WMT 2014英语到法语的翻译任务中，我们的模型建立了一个新的单模型最新BLEU得分为41.8，在8个GPU上续航3.5天，这是最好的模型训练成本的一小部分文献。我们通过将其成功应用于英语选区解析大型和有限的训练数据，表明Transformer能够很好地概括其他任务。

## 方法

大多数有竞争力的神经序列转导模型具有编码器 - 解码器结构。编码器将符号表示的输入序列 $$\left(x_{1}, \dots, x_{n}\right)$$ 映射到连续表示的序列 $$\mathbf{z}=\left(z_{1}, \dots, z_{n}\right)$$ 。 给定 $$z$$ ，解码器然后一次一个元素地生成符号的输出序列 $$\left(y_{1}, \dots, y_{m}\right)$$ 。

Transformer遵循这种整体架构，使用堆叠的自注意和逐点，完全连接的层，用于编码器和解码器，分别如图1的左半部分和右半部分所示。

![](../../.gitbook/assets/image%20%28125%29.png)



### Encoder and Decoder Stacks

#### Encoder

编码器由N = 6个相同层的堆栈组成。 每层都有两层。第一种是多头自我关注机制，第二种是简单的，位置完全连接的前馈网络。 我们在两个子层中的每一个周围使用残差连接\[11\]，然后是层归一化\[1\]（即针对每层的输出跨通道归一化）。也就是说，每个子层的输出是： $$Layernorm(x+\text { Sublayer }(x))$$ 。为了促进这些残差连接，模型中的所有子层以及嵌入层都生成 $$d_{\text { model }}=512$$ 的输出。

#### Decoder

解码器还由N = 6个相同层的堆栈组成。 除了每个编码器层中的双胞胎层之外，解码器还插入第三子层，其在编码器堆栈的输出上执行多头注视。 与编码器类似，我们在每个子层周围使用残余连接，然后进行层规范化。我们还修改了解码器堆栈中的自注意子层，以防止位置出现在后续位置。 此掩码与输出嵌入被一个位置偏移的事实相结合，确保了 $$i$$ 的预测仅依赖于小于 $$i$$ 的位置处的已知输出。

### Attention

注意功能可以被描述为将query和一组key-value对映射到output，其中query，key，value和output都是向量。输出被计算为值的加权总和，其中分配给每个值的权重由query与相应key的兼容性函数来计算。

![](../../.gitbook/assets/image%20%28171%29.png)

### Scaled Dot-Product Attention



